<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Offensive Language Identification for Indo-Aryan Languages: Offensive Language Identification for Indo-Aryan Languages</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Offensive Language Identification for Indo-Aryan Languages
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Offensive Language Identification for Indo-Aryan Languages </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="notes_main"></a>
Overview</h1>
<ul>
<li>We have 13k tweets collected which are a mixture of Offensive and non offensive tweets</li>
<li>Count of offensive tweets : 2418</li>
<li>count of non offensive tweets : 10577</li>
<li>Along with Offensive and non Offensive tweets we do have a 2 more levels of annotations<ul>
<li>if the tweets are offensive they are sub-categorized into Targeted insult (TIN) and un-targeted insult (UNT)</li>
<li>if the tweets are identified as TIN, they are sub-categorized into Individual (IND), Group(GRP), and Others (OTH).</li>
</ul>
</li>
<li>The data was then pre-processed (training and testing) these pre-processed data was then fed to the model for training and testing was done on the unseen data.</li>
<li>Through statistical significance testing using pairwise T-tests and Mann-Whitney U we have found out that:<ul>
<li>Level A classification : LSTM is the best performing model.</li>
<li>Level B classification : Random Forests Classifier and Support Vector classifier are the best performing models</li>
<li>Level C classification : Random Forest Classifier is the best performing model.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="model_main"></a>
Classical Model Training</h1>
<p >This project is mainly focused on the identifying the offensive tweets in low resource languages, we have chosen Marathi as our language of interest. Models like Random forest classifier, Decision tree classifier, Support vector classifier, Multinomial naive byes were used for training nad testing purposes.</p><ul>
<li>Decision Tree Classifier<ul>
<li>Decision tree builds classification in the form of a tree structure while breaking the dataset in smaller and smaller subsets while incrementing the structure simultaneously. In our dataset, for level A, a decision tree would have to break the dataset in “Offensive” and “Non-Offensive”. Similarly for Level B and Level C, a decision tree would have the task to form a tree structure. With this background, a Decision Tree model was trained.</li>
<li>Hyper-parameter tuning: The parameters used for tuning are max_depth, min_samples_leaf, criteria(gini, entropy).</li>
<li>This was run for 3 cross validations and for 10 iterations each. These parameters were selected according to how the model performed on previous experiments.</li>
</ul>
</li>
<li>Random Forest Classifier<ul>
<li>A Random Forest classifier estimates based on the combination of different Decision Trees. We can say that it fits a number of decision trees on various samples of the dataset. Each tree in the forest is built on random best set of features and this is what makes Random Forest one of the good performing algorithms while working with Natural Language Processing.</li>
<li>For hyperparameter tuning, the max_features selected were “auto” and “sqrt”, max_depth was selected from a range, min_sample_split and min_sample_leaf were hardcoded and a grid was formed.</li>
<li>Bootstrap (True/False) is used to select samples for training each tree</li>
</ul>
</li>
<li>Multinomial Naive Bayes<ul>
<li>MNB has been known for performing better with text favoured tasks. Naive Bayes can be put as a simple text classification algorithm which is based on the probability of the events occurring such that there is no interdependence between the variables. One good feature of Naive Bayes is that it performs well with less training data as well, and in our case while we were working in phases of generating a low resource language dataset, we thought that it would perform good.</li>
<li>Here, for hyperparameter tuning, we are using the alpha values ranging from 1 to 5 forming a grid to be tuned with 3 cross-validations and 100 iterations.</li>
</ul>
</li>
<li>Support Vector Classifier<ul>
<li>SVM algorithm determines the best decision boundary between the vectors. It basically decides where to draw the best line that divides the space in distinctive subspaces.Our approach was to find vector representation which can encode as much information as possible and then we apply the SVM algorithm for classification.</li>
<li>The class weight used for SVC is ‘balanced’</li>
<li>The grid for SVC consists of kernel (linear, polynomial, rbf, sigmoid) and gamma as scale/auto.</li>
<li>3 cross validations with 100 iterations. </li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="LSTM_main"></a>
LSTM Training</h1>
<ul>
<li>Why did we choose LSTM for the problem at hand<ul>
<li>Humans don't think from scratch, when we read an article, we understand each word based on the previous word. Traditional Neural networks fail to do this.</li>
<li>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.</li>
<li>Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997).</li>
<li>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! This exact behavior helps us tremendously as we are working on low resource languages and a model that remembers information/features for a long period of time is a necessity.</li>
</ul>
</li>
<li>Model Summary:<ul>
<li>An embedding layer with parameters -<ul>
<li>Input dim = vocabulary size</li>
<li>Output dim = 32</li>
<li>Input length = size of the padded sequence</li>
<li>Mask_zer0 = True to ignore 0</li>
</ul>
</li>
<li>An LSTM layer with parameter -<ul>
<li>Units = 100 (the resulting accuracy is almost same regardless of this value.</li>
</ul>
</li>
<li>Three dense layers</li>
<li>An output dense layer with parameters<ul>
<li>Units = 2 and 3 for level A,B and C respectively.</li>
<li>Activation = softmax ( for multi classification problem)</li>
</ul>
</li>
<li>Compilation with parameters<ul>
<li>Loss = categorical cross entropy</li>
<li>Optimizer = adam</li>
<li>Metrics = accuracy </li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="dependency_main"></a>
Libraries</h1>
<ul>
<li>Python</li>
<li>Pandas</li>
<li>Numpy</li>
<li>Matplotlib</li>
<li>Seaborn</li>
<li>Scikit Learn</li>
<li>Tensorflow, Keras</li>
<li>Jupyter notebook</li>
<li>Scipy</li>
<li>Pingouin </li>
</ul>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.2
</small></address>
</body>
</html>
