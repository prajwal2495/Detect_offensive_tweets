{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('TrainingDataset.csv')\n",
    "testing_data = pd.read_csv('TestingDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>१९६६ साली छत्रपती शिवाजी महाराज या जागतिक दर्ज...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>भक्तांची आधुनिक झासी कंगना रानावत हिचे झाशीच्य...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>शांत राहिले की ही बाळासाहेबांची सेना नाही अन् ...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>फडणवीस सरकार असताना आरक्षनाचे राजकारण करुण तों...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>मादरचोद रांडाची औलाद आहात तुम्हीं साले गुंडे म...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  subtask_a subtask_b  \\\n",
       "0  १९६६ साली छत्रपती शिवाजी महाराज या जागतिक दर्ज...  Offensive       TIN   \n",
       "1  भक्तांची आधुनिक झासी कंगना रानावत हिचे झाशीच्य...  Offensive       TIN   \n",
       "2  शांत राहिले की ही बाळासाहेबांची सेना नाही अन् ...  Offensive       TIN   \n",
       "3  फडणवीस सरकार असताना आरक्षनाचे राजकारण करुण तों...  Offensive       TIN   \n",
       "4  मादरचोद रांडाची औलाद आहात तुम्हीं साले गुंडे म...  Offensive       TIN   \n",
       "\n",
       "  subtask_c  \n",
       "0       GRP  \n",
       "1       GRP  \n",
       "2       GRP  \n",
       "3       GRP  \n",
       "4       GRP  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# pre-processing the data\n",
    "def clean_text(row, options):\n",
    "\n",
    "    if options['lowercase']:\n",
    "        row = str(row).lower()\n",
    "\n",
    "    if options['strip_spaces']:\n",
    "        row = str(row).strip()\n",
    "\n",
    "    if options['remove_url']:\n",
    "        row = str(row).replace('http\\S+|www.\\S+', '')\n",
    "\n",
    "    if options['remove_mentions']:\n",
    "        row = re.sub(\"@[A-Za-z0-9]+\",\"@USER\",row)\n",
    "\n",
    "    if options['remove_newline']:\n",
    "        row = re.sub(r'\\n',' ',row)\n",
    "\n",
    "    if options['remove_tab']:\n",
    "        row = re.sub(r'\\t',' ',row)\n",
    "\n",
    "    if options['remove_english']:\n",
    "        row = re.sub(\"[A-Za-z0-9]+\",\"\",row)\n",
    "\n",
    "    if options['add_USER_tag']:\n",
    "        row = re.sub(\"@\",\"@USER\",row)\n",
    "\n",
    "    if options['remove_specials']:\n",
    "        row = re.sub('[+,-,_,=,/,<,>,!,#,$,%,^,&,*,\\\",:,;,.,' ',\\t,\\r,\\n,\\',|]','',row)\n",
    "\n",
    "    if options['remove_Quotes']:\n",
    "        row = re.sub(\"'\",\"\",row)\n",
    "\n",
    "    return row\n",
    "\n",
    "clean_config = {\n",
    "    'remove_url': True,\n",
    "    'remove_mentions': True,\n",
    "    'decode_utf8': True,\n",
    "    'lowercase': True,\n",
    "    'remove_english': True,\n",
    "    'remove_specials': True,\n",
    "    'add_USER_tag': True,\n",
    "    'remove_newline':True,\n",
    "    'remove_tab':True,\n",
    "    'strip_spaces':True,\n",
    "    'remove_Quotes':True\n",
    "    }\n",
    "\n",
    "\n",
    "def demoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # csv file\n",
    "    #input_file = 'Data/Fully_Annotated.csv'\n",
    "\n",
    "    dataset_df = testing_data\n",
    "\n",
    "    #dataset_df = pd.DataFrame(dataset)\n",
    "\n",
    "    dataset_df = dataset_df[[\"tweet\",'subtask_a','subtask_b','subtask_c']]\n",
    "    #, \"subtask_a\", \"subtask_b\", \"subtask_c\"\n",
    "\n",
    "    #lowe case conversion\n",
    "    dataset_df['tweet'] = dataset_df['tweet'].str.lower()\n",
    "\n",
    "    # calling pre-processing function\n",
    "    dataset_df['tweet'] = dataset_df['tweet'].apply(clean_text, args=(clean_config,))\n",
    "\n",
    "    #stripping leading and trailing whitespaces\n",
    "    dataset_df['tweet'] = dataset_df['tweet'].str.strip()\n",
    "\n",
    "    #remove emojis - not working\n",
    "    dataset_df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "    #remove emojis - working\n",
    "    dataset_df['tweet'] = dataset_df['tweet'].apply( lambda x : demoji(x))\n",
    "\n",
    "    # convert df to csv\n",
    "    dataset_df.to_csv('./testing_cleaned.csv',index = False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_cleaned = pd.read_csv(\"training_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10988)\t0.3028838009470212\n",
      "  (0, 17722)\t0.29449079111026455\n",
      "  (0, 9467)\t0.3028838009470212\n",
      "  (0, 6138)\t0.28763320275240184\n",
      "  (0, 13195)\t0.3028838009470212\n",
      "  (0, 14795)\t0.2442779258081979\n",
      "  (0, 6953)\t0.2270980532885134\n",
      "  (0, 18890)\t0.3028838009470212\n",
      "  (0, 10857)\t0.19263397363819082\n",
      "  (0, 10934)\t0.15426723908734097\n",
      "  (0, 17666)\t0.15908360539244384\n",
      "  (0, 9415)\t0.17857213453181142\n",
      "  (0, 6113)\t0.17108717964377632\n",
      "  (0, 13153)\t0.12165637184914382\n",
      "  (0, 14790)\t0.2213000449458429\n",
      "  (0, 6950)\t0.2223903416659931\n",
      "  (0, 18889)\t0.3028838009470212\n",
      "  (1, 13874)\t0.2656398347544727\n",
      "  (1, 11536)\t0.22836640756141807\n",
      "  (1, 9158)\t0.2656398347544727\n",
      "  (1, 15842)\t0.2656398347544727\n",
      "  (1, 9565)\t0.2656398347544727\n",
      "  (1, 11670)\t0.259454077191973\n",
      "  (1, 16093)\t0.259454077191973\n",
      "  (1, 6247)\t0.2421229205207951\n",
      "  :\t:\n",
      "  (4621, 18391)\t0.10878160445193925\n",
      "  (4621, 14672)\t0.11892852827277796\n",
      "  (4621, 11784)\t0.09998713841673679\n",
      "  (4621, 5471)\t0.10116240328691807\n",
      "  (4621, 4048)\t0.08620335178716713\n",
      "  (4621, 8654)\t0.13175080424885457\n",
      "  (4621, 2519)\t0.11291706991547598\n",
      "  (4622, 7131)\t0.2738448568171038\n",
      "  (4622, 8664)\t0.2738448568171038\n",
      "  (4622, 10926)\t0.2738448568171038\n",
      "  (4622, 19053)\t0.2738448568171038\n",
      "  (4622, 2652)\t0.2738448568171038\n",
      "  (4622, 19051)\t0.2611492008162531\n",
      "  (4622, 17903)\t0.2611492008162531\n",
      "  (4622, 740)\t0.2611492008162531\n",
      "  (4622, 18248)\t0.2521414895325356\n",
      "  (4622, 18240)\t0.19476090759827588\n",
      "  (4622, 8662)\t0.19235115424541538\n",
      "  (4622, 14709)\t0.18248554435559716\n",
      "  (4622, 724)\t0.1625221435938494\n",
      "  (4622, 10925)\t0.1833434429616979\n",
      "  (4622, 7109)\t0.1543071668128956\n",
      "  (4622, 17840)\t0.10971873198929086\n",
      "  (4622, 8654)\t0.36008892799253894\n",
      "  (4622, 2607)\t0.15198674687399108\n"
     ]
    }
   ],
   "source": [
    "training_data_cleaned.head(5)\n",
    "tf = TfidfVectorizer(ngram_range=(1,2))\n",
    "X = tf.fit_transform(training_data_cleaned['tweet'].values.astype('U'))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = training_data_cleaned.drop(['subtask_a','subtask_b','subtask_c'],axis=1).values.astype('U')\n",
    "y = training_data_cleaned['subtask_c'].values.astype('U')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3467, 19089)\n",
      "(3467,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1156, 19089)\n",
      "(1156,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3467x19089 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 54443 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RF = RandomForestClassifier(random_state=42)\n",
    "    # Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "    # Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 220, num=11)]\n",
    "    #max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "    # Random search of parameters, using 3 fold cross validation,\n",
    "    # search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=RF, param_distributions=random_grid, n_iter=100, cv=3, verbose=2,\n",
    "                                   random_state=42, n_jobs=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>राम कदम वागण्यात नाही तर बोलण्यात चुकला बीजेपी...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>हीच का तुमची शिवसेने चि शिकवण आपली आई म्हणजे द...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>हे वाचा गाढवांनो आणि हे ही सांगा की तुमच्या मॅ...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>भक्त आंधळे असतात मूर्खा ना काही कळत नाही</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>वर्षे गोट्या खेडत बसले होते काय साहेब तुम्ही क...</td>\n",
       "      <td>Offensive</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  subtask_a subtask_b  \\\n",
       "0  राम कदम वागण्यात नाही तर बोलण्यात चुकला बीजेपी...  Offensive       TIN   \n",
       "1  हीच का तुमची शिवसेने चि शिकवण आपली आई म्हणजे द...  Offensive       TIN   \n",
       "2  हे वाचा गाढवांनो आणि हे ही सांगा की तुमच्या मॅ...  Offensive       TIN   \n",
       "3           भक्त आंधळे असतात मूर्खा ना काही कळत नाही  Offensive       TIN   \n",
       "4  वर्षे गोट्या खेडत बसले होते काय साहेब तुम्ही क...  Offensive       TIN   \n",
       "\n",
       "  subtask_c  \n",
       "0       GRP  \n",
       "1       GRP  \n",
       "2       GRP  \n",
       "3       GRP  \n",
       "4       IND  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data_cleaned = pd.read_csv('testing_cleaned.csv')\n",
    "testing_data_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_cleaned.head(5)\n",
    "unseen_y = testing_data_cleaned['subtask_c']\n",
    "unseen_X = tf.transform(testing_data_cleaned['tweet'].values.astype('U'))\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_TFIDF(X_train,y_train,X_test,y_test,unseen_X,unseen_y,model):\n",
    "    print(\"TFIDF + \", model)\n",
    "    #model = make_pipeline(TfidfVectorizer(ngram_range=(1, 5)), model)\n",
    "\n",
    "#     X_train = train_data['tweet'].values.astype('U')\n",
    "#     y_train = train_data['subtask_a'].values.astype('U')\n",
    "\n",
    "#     X_test = test_data['tweet'].values.astype('U')\n",
    "#     y_test = test_data['subtask_a'].values.astype('U')\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    labels = model.predict(X_test)\n",
    "\n",
    "    print(\"training Accuracy:\", metrics.accuracy_score(y_test, labels) * 100)\n",
    "\n",
    "    cm = confusion_matrix(y_test, labels)\n",
    "    print(\"Confusion matrix\\n\", cm)\n",
    "    print(classification_report(y_test, labels, digits=4))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    test_labels = model.predict(unseen_X)\n",
    "    \n",
    "    print(\"testing accuracy:\",metrics.accuracy_score(unseen_y,test_labels) * 100)\n",
    "    cm = confusion_matrix(unseen_y, test_labels)\n",
    "    print(\"Confusion matrix\\n\", cm)\n",
    "    print(classification_report(unseen_y, test_labels, digits=4))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF +  RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42),\n",
      "                   n_iter=100, n_jobs=-1,\n",
      "                   param_distributions={'bootstrap': [True, False],\n",
      "                                        'max_depth': [10, 31, 52, 73, 94, 115,\n",
      "                                                      136, 157, 178, 199, 220],\n",
      "                                        'max_features': ['auto', 'sqrt'],\n",
      "                                        'min_samples_leaf': [1, 2, 4],\n",
      "                                        'min_samples_split': [2, 5, 10],\n",
      "                                        'n_estimators': [200, 400, 600, 800,\n",
      "                                                         1000, 1200, 1400, 1600,\n",
      "                                                         1800, 2000]},\n",
      "                   random_state=42, verbose=2)\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwalkrishn/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Accuracy: 75.69204152249135\n",
      "Confusion matrix\n",
      " [[ 20  15   1  15]\n",
      " [ 11 230  10 158]\n",
      " [  2   7   2   4]\n",
      " [  1  57   0 623]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GRP     0.5882    0.3922    0.4706        51\n",
      "         IND     0.7443    0.5623    0.6407       409\n",
      "         OTH     0.1538    0.1333    0.1429        15\n",
      "         nan     0.7788    0.9148    0.8413       681\n",
      "\n",
      "    accuracy                         0.7569      1156\n",
      "   macro avg     0.5663    0.5007    0.5239      1156\n",
      "weighted avg     0.7501    0.7569    0.7449      1156\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_TFIDF(X_train,y_train,X_test,y_test,unseen_X,unseen_y,rf_random)\n",
    "print(rf_random.best_params_)\n",
    "print(\"End of RFC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
